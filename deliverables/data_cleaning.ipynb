{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import word2number\n",
    "from word2number import w2n\n",
    "# from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory = pd.read_csv(\"C:/Users/Vedant/Desktop/Assignments-1/deliverables/inventory.csv\", encoding='utf-8')\n",
    "\n",
    "products = pd.read_csv(r\"C:/Users/Vedant/Desktop/Assignments-1/deliverables/products.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_json('C:/Users/Vedant/Desktop/Assignments-1/deliverables/customers.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 Excel files to read with 8 threads.\n",
      "✅ Processed file 1/2\n",
      "✅ Processed file 2/2\n",
      "\n",
      "📊 Total records combined: 140675\n"
     ]
    }
   ],
   "source": [
    "sales_folder = \"C:/Users/Vedant/Desktop/Assignments-1/deliverables/sales\"\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        return pd.read_excel(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_sales_parallel(folder_path, max_threads=8, limit=None):\n",
    "    xlsx_files = sorted([\n",
    "        os.path.join(folder_path, f)\n",
    "        for f in os.listdir(folder_path)\n",
    "        if f.endswith(\".xlsx\")\n",
    "    ])\n",
    "    \n",
    "    if limit is not None:\n",
    "        xlsx_files = xlsx_files[:limit]\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files to read with {max_threads} threads.\")\n",
    "\n",
    "    all_sales = []\n",
    "    with ThreadPoolExecutor(max_threads) as executor:\n",
    "        future_to_file = {executor.submit(read_file, f): f for f in xlsx_files}\n",
    "        for i, future in enumerate(as_completed(future_to_file), 1):\n",
    "            result = future.result()\n",
    "            if not result.empty:\n",
    "                all_sales.append(result)\n",
    "            print(f\"✅ Processed file {i}/{len(xlsx_files)}\")\n",
    "\n",
    "    combined_df = pd.concat(all_sales, ignore_index=True)\n",
    "    print(f\"\\n📊 Total records combined: {len(combined_df)}\")\n",
    "    return combined_df\n",
    "\n",
    "# 🔁 Example usage: Load only first 20 files\n",
    "combined_sales_df = load_sales_parallel(sales_folder, max_threads=8, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    # Clean column names\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()              \n",
    "                 .str.lower()               \n",
    "                 .str.replace(\" \", \"_\")     \n",
    "                 .str.replace(r\"[^\\w_]\", \"\", regex=True) \n",
    "    )\n",
    "    # Clean string-type cells\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.strip().str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "combined_sales_df = clean_dataframe(combined_sales_df)\n",
    "inventory = clean_dataframe(inventory)\n",
    "products = clean_dataframe(products)\n",
    "customers = clean_dataframe(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_number(val):\n",
    "    try:\n",
    "        return int(val)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return word2number.word_to_num(val)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "def lvl1cleaning(df):\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    print(f\"\\nCONVERTING NUMBER WORDS TO INTEGER IN INT COL: {numeric_cols}\")\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].apply(lambda x: convert_to_number(x) if isinstance(x, str) else x)\n",
    "\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "    print(\"\\n MISSING VALUES AFTER FILLING NAN AS 0\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if not missing.empty:\n",
    "        print(\"COLUMNS WITH MISSING VALUES:\")\n",
    "        print(missing.sort_values(ascending=False))\n",
    "    else:\n",
    "        print(\"No missing values.\")\n",
    "\n",
    "        \n",
    "    \n",
    "def quick_eda(df, show_value_counts=False, value_count_limit=10):\n",
    "    print(f\"ROWS: {df.shape[0]}, COLUMNS: {df.shape[1]}\")\n",
    "    print(f\"\\n COLUMNS & TYPES : \\n {df.dtypes}\")\n",
    "    \n",
    "    print(\"\\n MISSING VALUES\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if not missing.empty:\n",
    "        print(\"COLUMNS WITH MISSING VALUES:\")\n",
    "        print(missing.sort_values(ascending=False))\n",
    "    else:\n",
    "        print(\"No missing values.\")\n",
    "    \n",
    "    print(f\"\\n UNIQUE VALUES PER COLUMN \\n {df.nunique()}\")\n",
    "    print(f\"\\n SAMPLE DATA \\n {df.head()}\")\n",
    "\n",
    "    print(\"\\n SUMMARY STATS (Numerical)\")\n",
    "    print(df.describe(include=[int, float]))\n",
    "    \n",
    "    print(\"\\n SUMMARY STATS (Categorical)\")\n",
    "    print(df.describe(include=[object, \"category\"]))\n",
    "    \n",
    "    if show_value_counts:\n",
    "        print(\"\\n VALUE COUNTS\")\n",
    "        for col in df.columns:\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals <= value_count_limit:\n",
    "                print(f\"\\n{col} (top {value_count_limit} categories)\")\n",
    "                print(df[col].value_counts(dropna=False).head(value_count_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROWS: 15000, COLUMNS: 6\n",
      "\n",
      " COLUMNS & TYPES : \n",
      " warehouse_id           object\n",
      "product_id             object\n",
      "stock_level            object\n",
      "reorder_level           int64\n",
      "avg_daily_sales         int64\n",
      "days_until_reorder    float64\n",
      "dtype: object\n",
      "\n",
      " MISSING VALUES\n",
      "COLUMNS WITH MISSING VALUES:\n",
      "stock_level    616\n",
      "dtype: int64\n",
      "\n",
      " UNIQUE VALUES PER COLUMN \n",
      " warehouse_id          15000\n",
      "product_id              100\n",
      "stock_level             293\n",
      "reorder_level            81\n",
      "avg_daily_sales          91\n",
      "days_until_reorder       10\n",
      "dtype: int64\n",
      "\n",
      " SAMPLE DATA \n",
      "   warehouse_id product_id stock_level  reorder_level  avg_daily_sales  \\\n",
      "0     w000_000    pid1000         ten             63               16   \n",
      "1     w000_001    pid1001         138             45               79   \n",
      "2     w000_002    pid1002          69             88               40   \n",
      "3     w000_003    pid1003          42             95               25   \n",
      "4     w000_004    pid1004         105             61               60   \n",
      "\n",
      "   days_until_reorder  \n",
      "0                 1.6  \n",
      "1                 1.7  \n",
      "2                 1.7  \n",
      "3                 1.7  \n",
      "4                 1.8  \n",
      "\n",
      " SUMMARY STATS (Numerical)\n",
      "       reorder_level  avg_daily_sales  days_until_reorder\n",
      "count   15000.000000     15000.000000        15000.000000\n",
      "mean       59.892133        55.336000            1.590447\n",
      "std        23.330220        26.268948            0.234256\n",
      "min        20.000000        10.000000            1.100000\n",
      "25%        40.000000        33.000000            1.400000\n",
      "50%        60.000000        55.000000            1.600000\n",
      "75%        80.000000        78.000000            1.800000\n",
      "max       100.000000       100.000000            2.000000\n",
      "\n",
      " SUMMARY STATS (Categorical)\n",
      "       warehouse_id product_id stock_level\n",
      "count         15000      15000       14384\n",
      "unique        15000        100         293\n",
      "top        w000_000    pid1000     fifteen\n",
      "freq              1        150         266\n",
      "\n",
      " VALUE COUNTS\n",
      "\n",
      "days_until_reorder (top 10 categories)\n",
      "days_until_reorder\n",
      "1.8    1963\n",
      "1.9    1945\n",
      "1.6    1941\n",
      "1.4    1860\n",
      "1.7    1845\n",
      "1.5    1789\n",
      "1.3    1788\n",
      "1.2    1203\n",
      "2.0     664\n",
      "1.1       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# quick_eda(combined_sales_df, show_value_counts=True)\n",
    "quick_eda(inventory, show_value_counts=True)\n",
    "# quick_eda(products, show_value_counts=True)\n",
    "# quick_eda(customers, show_value_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONVERTING NUMBER WORDS TO INTEGER IN INT COL: ['reorder_level', 'avg_daily_sales', 'days_until_reorder']\n",
      "\n",
      " MISSING VALUES AFTER FILLING NAN AS 0\n",
      "COLUMNS WITH MISSING VALUES:\n",
      "stock_level    616\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "lvl1cleaning(inventory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
